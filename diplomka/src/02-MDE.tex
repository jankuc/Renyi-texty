\chapter{Minimum Distance  Estimation}\label{ch:MDE}
\section{\ren Divergence}
%Throughout this work we use \ren divergence in various 
Let $\mathcal{P}$ be a set of probability distributions on measurable space $(\mathcal{X, A}), \: P,Q \in \mathcal{P},$ and let $p,q$ be their respective probability density functions. Furthermore we assume that $\mathcal{P}$ is dominated by $\sigma$-finite measure $\lambda$.
%Let $\mathbf{X}_n = (X_1, \ldots ,X_n)$ i.i.d. observations  with distribution $P_{0} \in \mathcal{F}$. 

%\begin{definition}
%Let $\phi : (0,\infty) \rightarrow \mathbb{R}$ is convex on $(0,\infty)$, strictly convex at point point $t = 1$ and let $\phi(1) = 0$. %Let us denote $\phi(0) := \lim_{t\rightarrow 0_+} \phi(t)$ and $\phi(infty)/\infty  := \lim_{t\rightarrow +\infty} \phi(t)/t.$
%
%
%\end{definition}


\begin{definition}
Let $\Phi$ be the class of all functions  $\phi(t)$, $\phi : (0,\infty) \rightarrow \mathbb{R},$ which are convex on $(0,\infty)$ and strictly convex at $t = 1$.  We define 
\begin{equation}
\phi(0) = \lim_{t \searrow 0} \phi(t), \quad \text{ and } \quad \frac{\phi(\infty)}{\infty} = \lim_{t\rightarrow \infty} \frac{\phi(t)}{t}.
\end{equation}
Suppose that $\phi(1) = 0$ and $\phi(0)$ exists. Then the {\em \phidiv} measure between the probability distributions $P$ and $Q$ is defined as 
\begin{equation}
\mathfrak{D}_\phi(P,Q) = \int_\mathcal{X} q(x) \phi \left( \frac{p(x)}{q(x)}\right) \,\mathrm{d}\lambda , \qquad \phi \in \Phi.
\label{eq:phiDiv}
\end{equation}
We define the following values
\begin{equation}
q(x) \phi\left( \frac{p(x)}{q(x)}\right) = 
\begin{cases}
q(x)\phi(0) & \text{ if } p(x) = 0,  \\
p(x) \dfrac{\phi(\infty)}{\infty} & \text{ if } q(x) = 0,
\end{cases}
\end{equation}
where we use convention "$0\cdot\infty = 0$".


\label{def:phiDiv}
\end{definition}
\noindent Example of \phidiv is the well-known  Kullback-Leibler divergence (or informational divergence) 
\begin{equation}
\mathfrak{D}_{KL}(P,Q) =  \int_\mathcal{X} p(x)\log\left( \frac{p(x)}{q(x)}\right) \mathrm{d}\lambda
\end{equation}
defined by $\phi(t) = t\log(t)$.

\begin{theorem}
Let $P,Q \in \mathcal{P}$ and let $\phi \in \Phi$ be differentiable at $t = 1$. Then
\begin{equation}
0 \leq \mathfrak{D}_\phi (P,Q) \leq \phi(0) + \frac{\phi(\infty)}{\infty},
\end{equation}
where
\begin{equation}
 \mathfrak{D}_\phi (P,Q) = 0 \quad \text{ if } P=Q, 
\end{equation}
and
\begin{equation}
\mathfrak{D}_\phi (P,Q) = \phi(0) + \frac{\phi(\infty)}{\infty} \quad \text{ if } S_P \cap S_Q = \emptyset,
\end{equation}
where $S_P, S_Q$  are supports of $P,Q$.
\end{theorem}



In this work we deal with even more general divergence presented by \ren in \cite{Renyi1961}. It was the first parametric generalization of $\mathfrak{D}_{KL}$, defined by
\begin{align*}
\mathfrak{R}_\alpha(P,Q) & = \frac{1}{\alpha-1} \log\int_\mathcal{X} p(x)^\alpha q(x)^{1-\alpha}\mathrm{d}\lambda , \qquad \alpha >0, \alpha \neq 1.
%& = \frac{1}{\alpha-1} \log \mathrm{E}_P\left[  \left( \frac{p(X)}{q(X)}\right)^{\alpha -1} \right] 
\end{align*}
Liese and Vajda \cite{LieseVajda1987} later extended it for all $\alpha \neq 0,1,$ by
\begin{equation}
\begin{aligned}
\mathfrak{R}_\alpha(P,Q)  = \frac{1}{\alpha(\alpha-1)} \log\int_\mathcal{X} p(x)^\alpha q(x)^{1-\alpha}\mathrm{d}\lambda.% = \frac{1}{\alpha(\alpha-1)} \log \mathrm{E}_P\left[  \left( \frac{p(X)}{q(X)}\right)^{\alpha -1} \right].
\label{eq:renDiv}
\end{aligned}
\end{equation}
In the following, expression \eqref{eq:renDiv} will be referred to as \ren divergence.
\ren divergence cannot be written in the form of $\phi-$divergence measure, however, it can be written in more general form called $(h,\phi)$-divergence introduced  in \cite{Menendez1995}. It is defined by  $\mathfrak{D}_\phi^h(P,Q) = h( \mathfrak{D}_{\phi}(P,Q))$, where  $\phi \in \Phi$ and $ h$ is differentiable, $h(0) = 0$, $h' (0) > 0 $, with  \[h : \left[0,\phi(0) + \lim_{x\rightarrow \infty} \frac{\phi(x)}{x} \right] \rightarrow [0,\infty).\] For completeness we present functions $h(t), \phi(t)$ for the \ren divergence:
\begin{align*}
h(t) = & \frac{1}{\alpha(\alpha-1)} \log\left[ \alpha(\alpha - 1)t + 1\right], \qquad \alpha \neq 0,1, \\
\phi(t) = & \frac{t^\alpha - \alpha(t-1) -1}{\alpha(\alpha -1)} \qquad \alpha \neq 0,1.
\end{align*}


\section{Minimum Distance Estimator}
In this section we consider a parametric family $\mathcal{F} = \lbrace F_\theta : \theta \in \Theta \subset \mathbb{R}^m \rbrace$, of distribution functions on $\mathbb{R}$, where $\Theta \subset \mathbb{R}^m$ is metric space with Euclidean distance 
\begin{equation}
	\rho (\theta_1,\theta_2) = \sqrt{(\theta_1-\theta_2)(\theta_1-\theta_2)^T}, \quad \theta_1, \, \theta_2 \in \Theta.
\end{equation}
Furthermore, we assume identifiability of the family $\mathcal{F}$, which means 
\begin{equation}
\theta_1 \neq \theta_2 \Rightarrow F_{\theta_1} \neq F_{\theta_2}
\end{equation}
and also that all of $F_\theta(x), \, x \in \mathbb{R}$ are measurable. Our estimators will be calculated on the basis of i.i.d.\nomenclature{i.i.d}{independent and identically distributed} observations $\mathbf{X}_n = (X_1, \ldots ,X_n)$ all distributed by distribution function $F_{\theta_0} \in \mathcal{F}$. Then $F_{\theta_0} \in \mathcal{F}$ is a true distribution function and $\theta_0 \in \Theta$ is a true parameter.

We are interested in non-parametric estimates of the unknown distribution $F_{\theta_0}$ based on $\mathbf{X}_n$, which are sequences of mappings $ \mathbb{R}^n \rightarrow \mathcal{F}(\mathbb{R}) $ or directly in estimates of the unknown parameter $\theta_0$ (point estimator) based on the same sample $\mathbf{X}_n$, which is sequence of functions $\hat{\theta}_n : \mathbb{R}^n \rightarrow \Theta$. If $\mathcal{F}$ is dominated by a $\sigma$-finite measure $\lambda$ and we write \RN density as
\begin{equation}
f_\theta = \dfrac{\mathrm{d} F_\theta}{\mathrm{d} \lambda},
\end{equation}
we might be also interested in estimator $\hat{f}_n : \mathbb{R}^n \rightarrow \lbrace f_\theta : \theta \in \Theta  \rbrace$ of unknown density $f_{\theta_0}$.

The error of point estimates $\hat{\theta}_n$ is evaluated by $\rho (\hat{\theta}_n,\theta_0)$, error of density estimates by  $\| f_{\hat{\theta}_n} - f_{\theta_0} \|$, where
\begin{equation}
\| f_{\theta_1} - f_{\theta_2} \| = \int{|f_{\theta_1} - f_{\theta_2}|\,\mathrm{d}\lambda}
\end{equation}
is $\mathrm{L}_1$-norm. For evaluating the deviation of distribution functions we need some distance $\mathfrak{D}(F,G) $ on $\mathcal{F}(\mathbb{R}).$

\begin{definition} % 1 
For a set of random variables $X_n$, the notation $X_n = o_p(1)$ means that $\lim_{n \rightarrow \infty } X_n = 0$ holds in probability. The notation $X_n = o_p(\varepsilon_n)$ means that $X_n/\varepsilon_n = o_p(1)$ for some sequence $\varepsilon_n \searrow 0$.
\end{definition} 

\begin{definition} % 2
We say that an estimator $\hat{F}_n$ is {\em consistent estimator} of $F_{\theta_0}$ in distance $\mathfrak{D}$, if $\mathfrak{D}(\hat{F}_n, F_{\theta_0}) = o_p (1)$ and is measurable in $\mathbf{X}_n$.
\end{definition}

%\begin{definition}
%Řekneme, že posloupnost náhodných veličin $\lbrace X_n \rbrace$ s distribučními funkcemi $\lbrace F_n \rbrace$ je omezená v pravděpodobnosti, pokud pro každé $\varepsilon > 0 $ existují $M$ a $N$ takové, že 
%	\begin{equation}
%		F_n(M) - F_n(-M) > 1-\varepsilon, \quad \forall n > N.
%	\end{equation}
%	Tuto skutečnost značíme $X_n = O_p(1)$.
%\end{definition} 

\noindent As an example of non-parametric estimator of cumulative distribution function (CDF) \nomenclature{CDF}{Cumulative distribution function} we can use empirical cumulative distribution function .

\begin{definition}
	Let $(X_1, \ldots,X_n)$ be i.i.d. observations. Then \emph{empirical distribution function} (ECDF) \nomenclature{ECDF}{Empirical cumulative distribution function} $F_n(x)$ is defined by 
	\begin{equation}
	F_n(x) = \frac{1}{n} \sum_{i=1}^n I_{(-\infty,x]}(X_i) \quad \forall x \in \mathbb{R},
		\label{eq:ecdf}
	\end{equation}
	where $I_{(a,b]}(t)$ is the indicator function defined by 
	\begin{equation}
	I_{(a,b]}(t) = 
		\begin{cases}
			1 & \mathrm{if } \: t \in (a,b], \\
			0 & \mathrm{otherwise}.
		\end{cases}		
	\end{equation}
	\label{def:ecdf}	
\end{definition}

\noindent Glivenko-Cantelli theorem states limit property of ECDF:

\begin{theorem}[Glivenko-Cantelli]
	Let $(X_1, \ldots,X_n)$ be i.i.d. random sample with distribution function $F$ and let $F_n(x)$ be ECDF of this sample. Then for all $n \in \mathbb{N}$ it holds, that
	\begin{equation}
	\lim_{n\rightarrow \infty} \sup_{x\in \mathbb{R}} |F(x) - F_n(x)| = 0  \qquad a.s.
	\end{equation}
	\label{theo:glivenko-cantelli}
\end{theorem}
\noindent Proof can be found in \cite{Devroye}.

Later, for analysing FNAL data in chapter \ref{ch:GoF}, we will need weighted empirical distribution function.
\begin{definition}
	Let us have the assumptions from previous definition. Moreover, let us have vector of sample weights $(w_1, \ldots,w_n) \in \mathbb{R}$. Then \emph{weighted empirical distribution function (WECDF)\nomenclature{WECDF}{Weighted empirical cumulative distribution function}} $F^w_n(x)$ is defined by 
	\begin{equation}
	F^w_n(x) = \frac{1}{W} \sum_{i=1}^n w_i I_{(-\infty,x]}(X_i) \qquad \forall  x \in \mathbb{R},\qquad \text{ where }  W = \sum_{i=1}^n w_i.
	\label{eq:wecdf} 
	\end{equation}	
	\label{def:wecdf}
\end{definition}
%
%\begin{theorem}[Glivenko-Cantelli]
%Nech\v{t} jsou $X_1,\dots,X_n$ stejně a nezávisle rozdělené reálné ná\-ho\-dné veličiny s distribuční funkcí $F$ a nechť $F_n(x)$ je empirická distribuční funkce. Pak pro každé $n\in \mathbb{N}$ a $\varepsilon>0$ platí
%
%\begin{equation}
%P\left(\sup_{x\in\mathbb{R}}|F(x)-F_n(x)|>\varepsilon\right) \leq 8(n+1)\exp{\left\{ \frac{-n{\varepsilon}^2}{32}\right\}}.
%\end{equation}
%Z Borel-Cantelliho lemma pak plyne
%\begin{equation}
%\lim_{n \rightarrow +\infty}\sup_{x \in \mathbb{R}}|F(x)-F_n(x)|=0  \quad \text{s. j.}
%\end{equation}
%\end{theorem}



\noindent Now we can define minimum distance estimator (MDE)\nomenclature{MDE}{Minimum distance estimator} and asymptotically minimum distance estimator (AMDE).

\begin{definition}\label{def-mde}
	Let $\mathfrak{D}(\cdot, \cdot) $ be distance on $\mathcal{F}$ and $\hat{F}_n$ an estimator of distribution function $F_{\theta_0}$. A point estimator $\hat{\theta}_n$ measurable in $\Theta$ satisfying the condition
	\begin{equation}
		\mathfrak{D}(\hat{F}_n, F_{\hat{\theta}_n}) = \inf_{\theta \in \Theta}(\hat{F}_n, F_{\theta})
		\label{eq:MDE}
	\end{equation}
	is said to be \emph{minimum distance estimator (MDE)}. If the family $\mathcal{F}$ is dominated by some $\sigma$-finite measure $ \lambda$, then estimator's \RN density $f_{\hat{\theta}_n}$ is said to be \emph{minimum distance probability density estimator}. If the estimator meets the condition
	\begin{equation}
		\mathfrak{D}(F_n, F_{\hat{\theta}_n}) - \inf_{\theta \in \Theta}(F_n, F_{\theta_0}) = o_p(n^{-\frac{1}{2}}), 
	\end{equation}
	 instead of condition \ref{eq:MDE}, the function $\hat{\theta}_n$ is said to be \emph{approximate minimum distance estimator (AMDE)} and the corresponding probability density function $f_{\hat{\theta}_n}$ is said to be \emph{approximate minimum distance probability density estimator}.
\end{definition}

\noindent Note that the rate $n^{-\frac{1}{2}}$ is not important and it can be replaced by any $\varepsilon_n$ tending to 0 sufficiently fast.

\section{Robustness}

Let $\mathcal{P} = \lbrace P_\theta : \theta \in \Theta \subset \mathbb{R}^m \rbrace$ be set of probability distributions on measurable space $\left(\mathcal{X},\mathcal{A}\right)$. Let $T: \mathcal{P} \rightarrow \mathbb{R}^m$ be fisher consistent functional, that means $T(P_\theta) = \theta$ for all $\theta \in \Theta$. We introduce convex mixtures of distributions.

\begin{definition}
	Let us have $\varepsilon \in [0,1]$ and $P, Q \in \mathcal{P}$. We denote convex mixture of distributions $P, Q$ with coefficient $\varepsilon$ by
	\begin{equation}
		P_\varepsilon(Q) = (1-\varepsilon)P + \varepsilon Q.
		\label{konvex-smes}
	\end{equation}
\end{definition}

\subsection{Influence  Function}

Here we define influence function by which we measure the effect of one measurement $x$ on the estimate obtained by functional $T$.

\begin{definition}
	Let $\delta_x$ denotes Dirac delta function at the point $x,\, x \in \mathcal{X}$. \emph{Influence function}\nomenclature{IF}{Influence  function} $\IF{x}$ of functionals $T \in \mathcal{P},\, P \in \mathcal{P}$, is then defined by
	\begin{equation}
		\IF{x} = \lim_{\varepsilon \rightarrow 0_+} \frac{T(P_\varepsilon(\delta_x)) - T(P)}{\varepsilon} = \lim_{\varepsilon \rightarrow 0_+} \frac{T((1-\varepsilon)P + \varepsilon\delta_x) - T(P)}{\varepsilon}.
	\end{equation} 
\end{definition}

\noindent We can see from the definition, that if $\IF{x}$ is not bounded then even one distant measurement can cause a total failure of the estimator $T$.

We introduce three measures of robustness from \cite{Antoch92}, which characterize some possible disruptions of our model. 

\begin{definition}
	By \emph{gross error sensitivity measure} of functional $T$ for probability distribution $P$ we mean function  $\gamma^*$ defined by 
	\begin{equation}
		\gamma^* = \sup_{x \in \mathcal{X}} |\IF{x}|.
	\end{equation}
\end{definition}
\noindent This function gives a notion of the worst possible effect of occurrence of gross error in data sample on the value of estimator.  From the point of robustness of the estimator, it is therefore desirable for the value of $\gamma^*$ to be finite. Estimators for which the value of $\gamma^*$ is finite are called B-robust.

\begin{definition}
	By \emph{local shift sensitivity measure} of functional $T$ for probability distribution $P$ we mean function  $\lambda^*$ defined by 
	\begin{equation}
			\lambda^* = \sup_{x,y \in \mathcal{X},x \neq y}  \left| \frac{\IF{y} - \IF{x}}{y-x} \right|.
	\end{equation}
\end{definition}

\noindent If the studied probability distribution $P$ is symmetrical w.r.t. point $x=0$, we can define another measure of robustness.

\begin{definition}
	\emph{Rejection point} $\rho^*$ for functional $T$ for probability distribution $P$ is defined by 
	\begin{equation}
			\rho^* = \inf_{x \in \mathcal{X}} \lbrace r>0 \, | \, \IF{x} = 0 \:\; \forall x,\, |x| > r\rbrace.
	\end{equation}
	If such constant $r$  does not exist, we set $\rho^* = + \infty.$ 
\end{definition}

\noindent If the rejection point $\rho^*$ is finite for some estimator, it results from its definition, that contamination of the estimator by measurement in the region  $\lbrace x \, | \, \IF{x} = 0 \rbrace$ does not affect the estimator in any way. If on the other hand the rejection point $\rho^*$ is not finite, it is desirable that at least
\begin{equation}
	\lim_{x \rightarrow \pm\infty} \IF{x} = 0.
\end{equation}

\subsection{\texorpdfstring{$M$}{M}-estimators}
$M$-estimators are generalization of maximum likelihood estimator (MLE). They are estimators defined by maximization or minimization of appropriate function $\rho(\cdot,\cdot):\mathcal{X}\times \Theta \rightarrow \mathbb{R}$. If we have parametric model $\mathcal{P} = \lbrace P_\theta : \theta \in \Theta\rbrace$, then $M$\emph{-estimator} of parameter $\theta$ is defined by 
\begin{equation}
	\hat{\theta}_n = M_n(P_n) = \arg \min_{\theta \in \Theta} \sum_{i=1}^n \rho(X_i,\theta) = \arg \min_{\theta \in \Theta} \mathrm{E}_{P_n}\left[ \rho(X_i,\theta) \right],
	\label{Modhad1}
\end{equation}
where $X_1,\ldots,X_n$ is random sample with probability distribution  $P_\theta$.

If the parametric model has probability density function $p_\theta$, then specific example of M-estimator is MLE which is solution of
\begin{equation}
	\hat{\theta}_{n, MLE} = \arg\min_{\theta \in \Theta} \sum_{i=1}^n \left( -\ln p_\theta(X_i)\right).
\end{equation}

If there exists
\begin{equation}
	\psi(x,\theta) = \frac{\partial}{\partial \theta} \rho(x,\theta), 
\end{equation}
then $M_n$ is the solution, or one of the solutions, of the equation 
\begin{equation}
	\sum_{i=1}^n \psi(X_i,\theta) = 0, \qquad \theta \in \Theta,
	\label{Modhad2}
\end{equation}
so 
\begin{equation}
	\frac{1}{n}\sum_{i=1}^n \psi(X_i,\theta) = \mathrm{E}_{P_n}\left[ \psi(X,\theta) \right] = 0, \qquad \theta \in \Theta.
	\label{Modhad25}
\end{equation}
On the other hand, not every solution of \ref{Modhad2} or \ref{Modhad25} has to be equivalent to solution of \ref{Modhad1}, because some may correspond to one of local maxima.

The functional $M$ which is relevant to $M_n$ is defined by 
\begin{equation}
	M(P) = \arg \min_{\theta \in \Theta} \mathrm{E}_{P}\left[ \rho(x,\theta) \right] = \arg \min_{\theta \in \Theta} \int_\mathcal{X} \rho(x,\theta) \, \mathrm{d}P(x),
	\label{Modhad3}
\end{equation}
or as a solution of equation 
\begin{equation}
\mathrm{E}_{P}\left[ \psi(x,\theta) \right] =  \int_\mathcal{X} \psi(x,\theta) \, \mathrm{d}P(x) = 0, \qquad \theta \in \Theta.
\label{Modhad4}
\end{equation}
For the estimator to be unambiguous, it is required for the equation \ref{Modhad4} or for the minimization \ref{Modhad3} to have a single solution.

Here we present theorem, which allows us to derive influence function for common \ren estimator.
\begin{theorem} 
Let $\psi(\cdot,\theta) =  \frac{\partial}{\partial \theta} \rho(\cdot,\theta)$ exist and let it be absolute continuous with respect to $\theta$. Let equation \eqref{Modhad3} have single solution $M(P)$. Then if it exists, the influence function $\mathrm{IF}(x;M,P)$ is derived by
\begin{equation}
 \text{IF}(x;M,P) = -\left(\int_{\mathcal{X}} \dot{\psi} (y,M(P)) \, \mathrm{d}P(y)\right)^{-1} \psi(x,M(P)),
\end{equation}
where $\dot{\psi} (y,M(P)) = \left[\left(\frac{\mathrm{d}}{\mathrm{d}\theta}\right)^\mathrm{T}\psi(y,\theta)\right]_{\theta = M(P)}$.
\label{theo:IF}
\end{theorem}
\noindent Proof can be found in \cite{Demut2010}.

\section{Decomposable Pseudodistances}

In this section we look into \ren pseudodistance. It will be clear from the following definition, that it is not typical metric distance, because we don't require symmetry nor triangle inequality. 

Let $\mathcal{P} = \lbrace P_\theta : \theta \in \Theta \subset \mathbb{R}^m \rbrace$ be set of probability distributions on measurable space $(\mathcal{X, A})$. Our estimates will be based on i.i.d. observations $\mathbf{X}_n = (X_1, \ldots ,X_n)$ with distribution $P_{0} \in \mathcal{F}$. Because are interested in robustness, we allow $P_{0} \not\in \mathcal{P}$. Therefore we define set $\mathcal{P}^+ = \mathcal{P} \cup \lbrace P_0 \rbrace$.

By $\Pemp$ we denote the class of all empirical distribution functions, not necessarily from $\mathcal{P}.$
		
		\begin{definition}
	We say that the mapping $\mathfrak{D}:\mathcal{P}\times\mathcal{P}^+ \rightarrow \mathbb{R}$ is a \emph{pseudodistance} between probability measures $P \in \mathcal{P}$ and $Q \in \mathcal{P}^+$ if it holds, that	
		\begin{equation}
			\mathfrak{D}(P_\theta,Q) \geq 0 \qquad \forall \theta \in \Theta, \: \forall Q \in \mathcal{P}^+
		\end{equation}
		and 		
		\begin{equation}
			\mathfrak{D}(P_{\theta_1},P_{\theta_2})=0 \; \Leftrightarrow \; \theta_1=\theta_2 \qquad \forall \theta_1,\: \theta_2 \in \Theta.
		\end{equation}	
	This pseudodistance $\mathfrak{D}$ is called \emph{decomposable} on $\mathcal{P}\times\mathcal{P}^+$ if there exist functionals 
		 $\mathfrak{D}^0:\mathcal{P}\rightarrow\mathbb{R}$, $ \mathfrak{D}^1:\mathcal{P}^+ \rightarrow \mathbb{R}$ and measurable mapping
		  $\rho_\theta : \mathbb{R}^d \rightarrow \mathbb{R}$, $ \theta \in \Theta$, so that for all $\theta \in \Theta$ and for all $Q \in \mathcal{P}^+$ the expectation $\int{\rho_\theta }\, \mathrm{d}Q$ exists and it holds, that
		\begin{equation}
			\mathfrak{D} (P_\theta, Q) = \mathfrak{D}^0 (P_\theta) + \mathfrak{D}^1 (Q) + \int \rho_\theta \, \mathrm{d}Q.
		\end{equation}
\end{definition}

\begin{definition}
	We say that a functional $T_\mathfrak{D}:\mathcal{Q} \rightarrow \Theta$, for $\mathcal{Q}=\mathcal{P}^+ \cup \mathcal{P}_{\text{emp}}$	defines \emph{minimum pseudodistance estimator} (min $\mathfrak{D}$-estimator) if $\mathfrak{D}(P_\theta,Q)$ is a decomposable pseudodistance on $\mathcal{P}\times\mathcal{P}^+$ and parameters $T_\mathfrak{D}(Q) \in \Theta$ minimize $\mathfrak{D}^0 + \int{\rho_\theta}\,\mathrm{d}Q$ on $\Theta$ for all $Q \in \mathcal{Q}$, that means
	\begin{equation}
		T_\mathfrak{D}(Q) = \arg\min_{\theta \in \Theta} \left[ \mathfrak{D}^0(P_\theta) + \int{\rho_\theta}\,\mathrm{d}Q \right] \quad \forall Q \in \mathcal{Q}.
	\end{equation}
\end{definition}
\noindent In particular, for the empirical distribution function $Q = P_n = \frac{1}{n}\sum_{i-1}^n \delta_{X_i} \in \mathcal{P}_{emp}$
\begin{equation}
	\hat{\theta}_{\mathfrak{D},n} =T_\mathfrak{D}(P_n)  = \arg\min_{\theta \in \Theta}\left[ \mathfrak{D}^0(P_\theta) + \dfrac{1}{n} \sum_{i=1}^n \rho_\theta (X_i) \right].
\end{equation}
\begin{theorem}
	Every min $\mathfrak{D}$-estimator 
	\begin{equation}
		\left[ \mathfrak{D}^0(P_\theta) + \dfrac{1}{n} \sum_{i=1}^n \rho_\theta (X_i) \right]
	\end{equation}
	is Fisher consistent in the sense that
\begin{equation}
	T_\mathfrak{D}(P_{\theta_0}) = \arg\min_{\theta \in \Theta} \mathfrak{D}(P_\theta, P_{\theta_0}) = \theta_0,\quad \forall \theta_0 \in \Theta.
\end{equation}
\end{theorem}
\begin{proof}
Consider any fixed $\theta_0 \in \Theta$. Then $\mathfrak{D}^1(P_{\theta_0})$ is finite constant and from the definition of pseudodistance we get
\begin{align*}
T_\mathfrak{D}(P_{\theta_0}) & = \arg\min_{\theta \in \Theta} \left[ \mathfrak{D}^0(P_\theta) + \int{\rho_\theta}\,\mathrm{d}Q \right] 
% & = \arg\min_{\theta \in \Theta} \left[ \mathfrak{D}^0(P_\theta) +\mathfrak{D}^1(P_{\theta_0}) + \int{\rho_\theta}\,\mathrm{d}Q \right] 
 = \arg\min_{\theta \in \Theta} \mathfrak{D}(P_\theta,P_{\theta_0}) 
= \theta_0.
\end{align*}
\end{proof}


\begin{theorem}\label{theo:renMDE}
Let for some $\beta>0$ it holds that
	\begin{equation}
			p^\beta, q^\beta,\ln{p} \in \mathrm{L}_1(Q), \quad \forall P \in \mathcal{P}, Q \in \mathcal{P^+}.
			\label{eq:betaCond}
	\end{equation}
	Then for all $\alpha$, $0 < \alpha \leq \beta$, and for $P \in \mathcal{P}, \; Q \in \mathcal{P^+} $ the expression
	\begin{equation}
		\mathfrak{R}_\alpha (P,Q) = \dfrac{1}{1+\alpha}\ln{\left( \int{p^\alpha \,\mathrm{d}P } \right)} +
		\dfrac{1}{\alpha (1+\alpha)}\ln{\left( \int{q^\alpha \,\mathrm{d}Q } \right)} -
		\dfrac{1}{\alpha} \ln{\left( \int{p^\alpha \,\mathrm{d}Q } \right)}
		\label{eq:renDecDist}
	\end{equation}
		represents the family of pseudodistances decomposable in the sense of
	\begin{equation*}
		\mathfrak{R}_\alpha (P,Q) = \mathfrak{R}_\alpha^0 (P) + \mathfrak{R}_\alpha^1 (Q) - \dfrac{1}{\alpha} \ln{\left( \int{p^\alpha \,\mathrm{d}Q } \right)},
	\end{equation*}	
	where
	\begin{equation*}
		\mathfrak{R}_\alpha^0 (P) = \dfrac{1}{1+\alpha}\ln{\left( \int{p^\alpha \,\mathrm{d}P } \right)}, \quad \mathfrak{R}_\alpha^1 (Q) = \dfrac{1}{\alpha (1+\alpha)}\ln{\left( \int{q^\alpha \,\mathrm{d}Q } \right)}.
	\end{equation*}
	Moreover, for $\alpha \searrow 0$ it holds
	\begin{equation*}
		\mathfrak{R}_0 (P,Q) = \lim_{\alpha \searrow 0} \mathfrak{R}_\alpha (P,Q) =  \int{\left( \ln{q} - \ln{p} \right)\,\mathrm{d}Q}.
	\end{equation*}
\end{theorem}
\noindent Proof can be found in \cite{Decomposable2011}. \\
If we use \ref{eq:renDecDist} for distributions $Q \in \mathcal{P}^+ \cup \Pemp$ we get minimum \ren distance estimator defined by

\begin{equation}
	T_{\mathfrak{R}_\alpha}(Q) =
	\begin{cases}
		 \arg \min_{\theta} \left[\frac{1}{1+\alpha} \ln(\int p_\theta^\alpha\, \mathrm{d}P_\theta) - \frac{1}{\alpha} \ln(\int p_\theta^\alpha\, \mathrm{d}Q) \right] & \text{if } 0 < \alpha \leq \beta, \\
		 \arg \min_{\theta} \left[- \ln(\int p_\theta\, \mathrm{d}Q) \right] & \text{if } \alpha = 0,
	\end{cases}	
\end{equation}
which is equivalent to 
\begin{equation}
	T_{\mathfrak{R}_\alpha}(Q) = 
	\begin{cases}
		 \displaystyle{ \arg \max_{\theta \in \Theta} \left[\ln\frac{\int p_\theta^\alpha\, \mathrm{d}Q}{(\int p_\theta^\alpha\, \mathrm{d}P_\theta)^{\frac{\alpha}{1+\alpha}}} \right] }& \text{ if } 0 < \alpha \leq \beta, \\[5mm]
		 \displaystyle{ \arg \max_{\theta \in \Theta} \left[\ln(\int p_\theta\, \mathrm{d}Q) \right] }& \text{ if } \alpha = 0.
	\end{cases}	
\end{equation}
If we denote
\begin{equation}
C_{\alpha}(\theta) = \biggl(\int p_{\theta}^{1+\alpha} \, \mathrm{d}\lambda\biggr)^{\frac{\alpha}{1+\alpha}} = \left( \int p_\theta^\alpha\, \mathrm{d}P_\theta \right)^{\frac{\alpha}{1+\alpha}},
\end{equation}
then we can write \ren estimator for $0<\alpha \leq \beta$ in the form of $M$-estimator
\begin{equation}
T_{\alpha}(Q) = \text{argmax}_{\theta} M_{\alpha}(Q,\theta), \qquad \text{where } \quad M_{\alpha}(Q,\theta) = \frac{\int p_{\theta}^{\alpha}\, \mathrm{d}Q}{C_{\alpha}(\theta)}.
\end{equation}

We are interested in estimators, in which we replace the hypothetical distribution $P_{\theta_0}$ in $\mathfrak{R}_\alpha(P_\theta, P_{\theta_0})$ by empirical distribution $P_n \in \Pemp$. In this case we can write minimum \ren distance estimator in the form

% It means that the family of minimum \ren pseudo-distance estimators defined by $\hat{\theta}_{n,\alpha} = T_{\mathfrak{R}_\alpha}(P_n)$ for $T_{\mathfrak{R}_\alpha}(Q) \in \Theta$ with $Q \in \mathcal{P}^+$ satisfies the condition

\begin{equation}
	\hat{\theta}_{\mathfrak{R}_\alpha,n} =
	\begin{cases}
		\displaystyle{ \arg \max_{\theta \in \Theta} C_\alpha\left( \theta \right)^{-1} \frac{1}{n} \sum_{i=1}^n p_{\theta}^{\alpha}\left( X_i \right) } & \text{if } 0 < \alpha \leq \beta, \\
		\displaystyle{ \arg \max_{\theta \in \Theta}  \frac{1}{n} \sum_{i=1}^n \ln p_{\theta}\left( X_i \right) } & \text{if } \alpha = 0.
	\end{cases}	
	\label{eq:renEstimator}
\end{equation}
In \cite{Vajda2009} author derives the influence function for minimum \ren pseudodistance estimator according to theorem \ref{theo:IF}. If we use notation 
\begin{center}
	\begin{tabular}{c c}
	$s_\theta = \dfrac{\mathrm{d}}{\mathrm{d}\theta} \ln p_\theta, \quad$ & $ \dot{s}_\theta = \left( \dfrac{\mathrm{d}}{\mathrm{d}\theta} \right)^T s_\theta,$ \\ 
	&\\
	$c_\alpha(\theta) = \dfrac{\int p_\theta^{1+\alpha}s_\theta \mathrm{d}\lambda}{\int p_\theta^{1+\alpha} \mathrm{d}\lambda}, \quad$ & $\dot{c}_\alpha(\theta)= \left( \dfrac{\mathrm{d}}{\mathrm{d}\theta} \right)^T c_\alpha(\theta),$  \\ 
	\end{tabular} 
\end{center}
the influence function can be written according to theorem \ref{theo:IF} as
\begin{equation}
	\mathrm{IF}(x;T_{\mathfrak{R}_\alpha},\theta) = -\mathbf{I}^{-1}_{\alpha}(\theta) \left[ p_\theta^\alpha(x) (s_\theta (x) - c_\alpha (x)) \right], 
	\label{eq:IF}
\end{equation}
where 
\begin{equation}
\mathbf{I}_{\alpha}(\theta) = \int{ \left[\dot{s}_\theta - \dot{c}_\alpha(\theta) - \alpha(s_\theta - c_\alpha(\theta))(c^T_\alpha(\theta) - s^T_\theta) \right] p_\theta^{1+\alpha} \mathrm{d}\lambda}.
\end{equation}

\subsection{Application to Normal Distribution}

Results for normal model including specific forms of estimator \eqref{eq:renEstimator} and influence function \eqref{eq:IF} were already presented in \cite{Vajda2009} and \cite{Demut2010}. We present them here for the completeness and since we use them in simulations in which we tested the estimators on data samples with different kind of contamination.

Minimum \ren distance estimator for $\alpha = 0$ coincides with maximum likelihood estimator
\begin{equation}
\hat{\theta}_{\mathfrak{R}_0,n} = \text{argmax}_{\theta} \frac{1}{n}\sum_{i=1}^n \ln \biggl[\frac{1}{\sqrt{2\pi \sigma^2}} \exp\biggl(-\frac{(X_i-\mu)^2}{2\sigma^2}\biggr)\biggr].
\end{equation}

\noindent Condition (\ref{eq:betaCond}) holds for all $\beta > 0$, therefore for $\alpha>0$ we can transform {\mRao} \eqref{eq:renEstimator} to

\begin{equation}
	\hat{\theta}_{\mathfrak{R}_\alpha,n} = \text{argmax}_{\theta} \frac{1}{n\sigma^{\alpha/(1+\alpha)}}\sum_{i=1}^n\exp \biggl(-\alpha\frac{(X_i-\mu)^2}{2\sigma^2}\biggr).
\end{equation}

\noindent Influence function for estimator of the standard deviation ($\hat{\theta} = \sigma$) with the known mean $\mu = 0$ is derived in \cite{Vajda2009} and has the form

\begin{equation}
	\text{IF}(x;T_{\mathfrak{R}_{\alpha}},\hat{\sigma}) = \frac{(1+\alpha)^{5/2}\hat{\sigma}}{2}\biggl[\biggl(\biggl(\frac{x}{\hat{\sigma}}\biggr)^2-\frac{1}{1+\alpha}\biggr) \exp\biggl(-\frac{\alpha x^2}{\hat{\sigma}^2}\biggr)\biggr].
\end{equation}
For the estimator of mean ($\hat{\theta} = \mu$) with fixed $\sigma$ the influence function takes the form
\begin{equation}
	\text{IF}(x;T_{\mathfrak{R}_{\alpha}},\hat{\mu}) = (1+\alpha )^{3/2} (x-\hat{\mu} ) \exp\left[{-\alpha\frac{(x-\hat{\mu} )^2}{2 \sigma ^2}}\right].
\end{equation}
Both functions are bounded for all possible $\alpha$, which means that the estimator is B-robust. Although we do not have finite rejection point $\rho^*$, we can see, that for $\alpha>0$ the limit 
\begin{equation}
	\lim_{x \rightarrow \pm\infty} \mathrm{IF}(x;T_{\mathfrak{R}_\alpha},\cdot) = 0
\end{equation}	
holds for influence functions for both parameters.	
	
	\section{Application to Specific Families}
	\input{src/02a-Application_to_specific_families.tex}

\section{Heuristic Approach}
There is a small inconvenience with the choice of "robust" $\alpha$ ($\alpha > 0.5$) in combination with small data samples, or very sparse or scattered data (data with high variance). This is illustrated in Figures \subref*{figJK:distRen} and \subref*{figJK:distRenPlate}, where we used a set of 10 random values $\mathbf{X} = (x_1, \ldots, x_{10}) \sim \mathrm{N}(0,2)$.

\begin{figure}[htb]
  \centering
  \subfloat[t][Values of \ren distance.]{
  	\includegraphics[width=.5\textwidth]{RenyiN02a07n10-eps-converted-to.pdf}%
  	\label{figJK:distRen}%
  }%
  \:%
  \subfloat[t][Values of \ren distance after averaging.]{
  	\includegraphics[width=.5\textwidth]{RenyiPlate-average-eps-converted-to.pdf}%
  	\label{figJK:distRenPlate}%
  }%
  \caption{\ren distance in case of $\mathrm{N}(0,2)$ distribution.}
  \end{figure}

Figure \subref*{figJK:distRen} shows the \ren distance between the empirical distribution of observations $\mathbf{X}$ and normal distribution with parameters $(\mu,\, \sigma)$ varying in the interval $[-4,4]\times[0,8].$ We can see that there is large shallow minimum around the point $(0.3, \, 2.3)$, but closer examination would show that all of the eight bright points close to the $y$ axis $(\sigma = 0.001)$ have steep minima with lower values of the \ren distance function. These areas specify distributions which correspond to Dirac $\delta-$functions $\delta_{a_i}(x)$, where $a_i = x_i$ for $i \in \{1,\ldots,10\}$. Because of these singular values of the distance function, \mRao\ would prefer these distributions instead of that describing all the data together. This unifying distribution is represented by the dark blue area around $(0.3, \, 2.3)$. Thus the estimator is so robust and the data are so sparse that the estimator always picks up some single data point as the only representative of the estimated distribution and assumes that the rest of the samples are outliers. 

To overcome this problem, we developed a method inspired by image processing. The idea is to blur the resulting image so that there are not sharp and deep minima (edges from the viewpoint of image processing). Blurring was performed via convolution with averaging or Gaussian mask. If we denote $\mathfrak{R}_\alpha(F_n, \mathrm{N}(\mu,\sigma))$  \ren distance between the empirical distribution $F_n$ and $\mathrm{N}(\mu,\sigma)$, the distance after averaging is 
\begin{equation}
\overline{\mathfrak{R}}_\alpha(F_n, \mathrm{N}(\mu_k,\sigma_l)) = \dfrac{1}{r^2}\sum_{i=k-r}^{k+r} \sum_{j=l-r}^{l+r} \mathfrak{R}_\alpha(F_n, \mathrm{N}(\mu_i,\sigma_j)) ,
\end{equation}
where $r$ is the radius of the averaging mask.

\ren distance after averaging is displayed in Figure \subref*{figJK:distRenPlate}. We can see that the former minima corresponding to the Dirac $\delta-$functions are flattened after blurring and their values are higher than the minimum at point $(0.3, \, 2.3)$.

Blurring not only flattens the image, but it also slightly moves the minima. To overcome this setback and refine the result we used two-step algorithm:
\begin{enumerate}
\item Minimize averaged distance $\overline{\mathfrak{R}}_\alpha(\cdot,\cdot)$,
\item Minimize \ren distance $\mathfrak{R}_\alpha(\cdot,\cdot)$ in the local minima chosen by the step 1.
\end{enumerate}
So we use the average \ren distance to find the wide local minimum which describes the data as overall distribution and then we calculate \ren distance to find the exact estimator. 

\begin{table}[htb] 
\begin{center}
\begin{tabular}{|c|c|c|}
\hline  
 &${\hat{\mu}}$ & ${\hat{\sigma}} $\\
\hline
\ren distance& 0.1742  &  0.3448 \\    
Averaged \ren distance& 0.0058  &  2.8532 \\ 
Two-step minimization& 0.0346  &  2.3832 \\
\hline
\end{tabular}
\end{center}
\caption{Mean value obtained from 100 repetitions of \ren estimators. Data: $(x_1,\ldots,x_{10}) \sim \mathrm{N}(0,2)$}
\label{tabJK:RenPla}
\end{table}
\noindent Table \ref{tabJK:RenPla} shows the differences between the used algorithms.  The first row corresponds to the use of unaltered \mRao. We can see, that the mean of the estimated parameter $\sigma$ is very small, much smaller than the one of the theoretical distribution. In the second row we used only the averaging algorithm. In this case the estimator prefers estimators with higher value of $\sigma$. Result of the two-step algorithm is shown in the third row. We can see, that it's results are closer to that of averaged version of the estimator, but are refined by the second step of the minimization algorithm.

\section{Computer Simulations}
Computer simulations on \ren distance and \ren distance estimators are described in \cite{Kucera2012} in details, here we only present a few tables to get the general idea about the estimators at hand. In Tables \ref{tab:MDE1}--\ref{tab:MDE6} we present the estimates of the parameters of  exponential distribution. Estimates were computed from data generated by the  convex mixture \eqref{konvex-smes} of exponential distributions $(1-\varepsilon)\mathrm{E}(0,1) + \varepsilon\mathrm{E}(0,10)$  with $\varepsilon$ different for each table. In the tables we can find the following values:
\begin{equation}
	m(\gamma) = \frac{1}{K}\sum_{i=1}^K \widehat{\gamma}_{i}, \qquad s(\gamma) = \sqrt{\frac{1}{K}\sum_{i=1}^K (\widehat{\gamma}_{i}-{m(\gamma)})^2},\\
	\label{stat1}
\end{equation}
where $\gamma $ is one of the estimated parameters. Moreover we tabulate the so called empirical relative efficiency 
\begin{equation}
	\mathrm{eref}(\gamma) = \sqrt{\dfrac{\frac{1}{K}\sum_{i=1}^K (\widehat{\gamma}_{\mathrm{0} ,i} - \gamma_{\mathrm{real}})^2}{\frac{1}{i}\sum_{k=1}^K (\widehat{\gamma}_{\alpha,i} - \gamma_{\mathrm{real}})^2}},
	\label{stat2}
\end{equation}
where $\gamma_\mathrm{real}$ is the value of the parameter of the contaminated distribution,  $K$ is the number of repetitions of the estimator and $\widehat{\gamma}_{0,i}$ is the MLE estimator of the exponential family parameter $\lambda$ for $\gamma = \lambda$ and minimal $x$ present in the dataset for $\gamma = \mu$.
\begin{figure}[htb]
\centering
\includegraphics[width=0.7\textwidth]{Eref-Exp-lambda-eps-converted-to.pdf}
\caption{Empirical relative efficiency $\mathrm{eref}(\lambda)$ of the \mRa-estimator of parameter $\lambda$ of exponential distribution for the convex mixture $(1-\varepsilon)\mathrm{E}(0,1) + \varepsilon\mathrm{E}(0,10)$ under different contaminations $\varepsilon$ and $n=500.$}
\label{fig:eref}
\end{figure}

The tables show the dependence of the \ren estimator on the sample size $n$ and the parameter $\alpha$ of the \mRao. For better visualisation we plot the empirical relative efficiency in Figure \ref{fig:eref}. We can see that the efficiency for given contamination $\varepsilon$ increases with increasing $\alpha$ rapidly up to a certain point and then increases more slowly or even starts to decrease. This can be observed mainly for the values $\varepsilon=0.05, 0.1$. In Tables \ref{tab:MDE1}--\ref{tab:MDE6} we can see decreasing efficiency with increasing  $\alpha$. After some experiments we decided that the value $\alpha \approx 0.2 $ is a good compromise between the efficiency and robustness of the estimator. All the tables here and in  \cite{Kucera2012} were generated using java application \texttt{JAmde} which was created specially for testing minimum distance estimators. See chapter \ref{ch:software} for details.
 
\newpage
\changetext{}{+12em}{-6em}{-6em}{}
\begin{landscape}
		%\input{tables/tab-L01-L010-both.tex}
		\input{tables/tab-E01-E010-both.tex}
		%\input{tables/tab-C01-C010-both.tex}	
%		\input{E01-shifted-tab.tex}
\end{landscape}
\changetext{}{-12em}{+6em}{+6em}{}





 