\chapter{Goodness of Fit Tests}
\label{ch:GoF}

Vyber promennych \\
testy zalozene na divergencnich vzdalenostech \\
poradi promennych zalozene na div. vzdalenostech\\
 MC vs data\\
signal vs background \\
nekam zaradit neco o testu h0 vs h1 (v CM se odkazuju na hypotezu) \\
neco o porovnani teoretickeho a root KS \\
\\
\\


Conclusions from this chapter should be used for two things. For 
\noindent Goal of this chapter is devising tools for measuring similarity of two data samples and for measuring their difference. There are number of two-sample goodness of fit  tests, e.g. Kolmogorov-Smirnov test, Cram\'{e}r-von Mises test and many others. 

We need two-sample goodness of fit  tests in which we can use WECDF \ref{eq:wecdf} or incorporate the data weights by some other means.

In this chapter we consider two i.i.d. weighted random samples $\mathbf{X}^F = (X^F_1,\ldots, X^F_m)$ and $\mathbf{X}^G = (X^G_1, \ldots, X^G_n)$ with respective weights 
\begin{align*}
\mathbf{w}^F = (w^F_1,\ldots, w^F_m), \quad w_i^F \in \mathbb{R} \text{ for }  i \in \lbrace 1, \ldots, m \rbrace &\text{ and} \\
\mathbf{w}^G = (w^G_1,\ldots, w^G_n), \:
\quad w_i^G \in \mathbb{R} \text{ for } i \in \lbrace 1, \ldots, n \rbrace. &
\end{align*}
%generated by probability distributions $F$ and $G$ respectively. From these two samples, we construct ECDFs $F_m(x),\, G_n(x)$  and WECDFs  $F^w_m(x),\, G^w_n(x).$

By $F_m(x) $ and $ G_n(x)$ we will denote ECDFs defined by samples $\mathbf{X}^F$ and $\mathbf{X}^G$ respectively and by $F^w_m(x)$ and $ G^w_n(x)$ we will denote WECDFs defined by these samples. We also define so called joint sample 
\begin{equation}
\mathbf{X}^{F\cup G} = (X^{F\cup G}_1, \ldots, X^{F\cup G}_{m+n}) = (X^{F}_1, \ldots, X^{F}_{m},X^G_1,\ldots, X^G_n) = \mathbf{X}^{F} \cup \mathbf{X}^{G}
\end{equation}
with weights 
\begin{equation}
\mathbf{w}^{F\cup G} = (w^{F\cup G}_1,\ldots, w^{F\cup G}_{m+n}) = (w^{F}_1,\ldots, w^{F}_{m},w^{G}_1,\ldots, w^{G}_{n}) = \mathbf{w}^{F} \cup \mathbf{w}^{G}
\end{equation}
and with its ECDF $H_{m+n}(x)$ and WECDF $H^w_{m+n}(x)$.

% First we are going to list tests and ranks which we used for analysing the data.

The two-sample problem is to test
\begin{equation}
H_0: F = G \quad\: \text{vs.}\quad H_1: F \neq G \quad\text{ with significance level } \alpha
\end{equation} 
Because we have to take into account weights associated with the data, the most convenient way is to use tests based on ECDFs which we can replace by WECDFs. 

Lot of tests (including those mentioned below) are devised for relatively small data samples $m,n$. There usually is some exact theoretical distribution for sample sizes below 10. Sample sizes around 20 and more are often enough to use appropriate limiting distribution. Because we are dealing with sample sizes beginning at $m,n \geq 2000$ we are quite confident of using these limiting distributions.

\section{Kolmogorov-Smirnov Test}
Kolmogorov-Smirnov goodness of fit test is based on Glivenko-Cantelli theorem \ref{theo:glivenko-cantelli}. It uses statistic $D_{m,n}$ defined as Kolmogorov distance between ECDFs of analysed samples
\begin{equation}
D_{m,n} = \sup_{x \in \mathbb{R}} |F_m(x) - G_n(x)|.
\end{equation}
If $H_0$ is true, then according to \ref{theo:glivenko-cantelli} it holds that $D_{m,n} \rightarrow 0$ almost surely for $m \rightarrow \infty$ and $n \rightarrow \infty$. Now we present theorem which can serve as a basis for exact test.

\begin{theorem}[Smirnov]
	Let us denote  
	\begin{equation}
	K(\lambda) = 1 - 2\sum_{k=1}^{\infty} (-1)^{k+1} \exp\left[ -2k^2\lambda^2\right].
	\end{equation}
	Then 
	\begin{equation}
	\lim_{m,n \rightarrow \infty} \mathrm{P}(\sqrt{\frac{mn}{m+n}}D_{m,n} < \lambda) = K(\lambda)
	\end{equation}
	holds for all $\lambda$.
\end{theorem}
\noindent Proof can be found in \cite{Smirnov1944}. 
As was stated above, we need to re-weight the MC entries, so they match the data events, therefore we have to replace the ECDFs in the statistic $D_{m,n}$ by appropriate WECDFs
\begin{equation}
D_{m,n} = \sup_{x \in \mathbb{R}} |F^w_m(x) - G^w_n(x)|.
\end{equation}

\section{Cram\'{e}r-von Mises}
Two sample Cram\'{e}r-von Mises goodness of fit test is essentially similar to aforementioned Kolmogorov-Smirnov test. But instead of Kolmogorov distance, it is based on the $\omega^2$ criterion
\begin{equation}
\omega^2 = \frac{mn}{m+n} \int_{-\infty}^\infty \left[F_m(x) - G_n(x) \right]^2 \,\mathrm{d} H_{m+n}(x).
\end{equation} 
Statistic derived from this integral described in \cite{Anderson1962} is
\begin{align}
T_{m,n} & = \frac{mn}{(m+n)^2}\left( \sum_{i=1}^m \left( F_m(X^F_i) - G_n(X^F_i)\right)^2 + \sum_{j=1}^n \left( F_m(X^G_j) - G_n(X^G_j)\right)^2 \right) \\
& = \frac{mn}{(m+n)^2} \sum_{i=1}^{m+n} \left( F_m(X^{F\cup G}_i) - G_n(X^{F\cup G}_i)\right)^2 .
\label{eq:CMstat} %\\
%& = \frac{mn}{m+n} \sum_{i=1}^{m+n} \frac{1}{m+n} \left( F_m(X^{F\cup G}_i) - G_n(X^{F\cup %G}_i)\right)^2. 
\end{align}
%Here 1/(m+n) in the sum is weight equal for all data and it is the amount added to the ECDF at each data point. Because we don't have equally weighted data, we have to replace the fraction by weight ${w_i^{F\cup G}}$ specific for each data point. We can write the weighted statistic as
%\begin{equation}
%T^w = \frac{mn}{(m+n)} \sum_{i=1}^{m+n} w_i^{F\cup G} \left( F_m(X^{F\cup G}_i) - G_n(X^{F\cup G}_i)\right)^2. 
%\end{equation}
The expected value $\mathrm{E} [T_{m,n}]$ and variance $\mathrm{Var} [T_{m,n}]$ are derived in \cite{Anderson1962} and  under the null hypothesis they are 
\begin{equation}
\mathrm{E} [T_{m,n}] = \frac{1}{6} + \frac{1}{6(m+n)}
\end{equation}
and
\begin{equation}
\mathrm{Var} [T_{m,n}] = \frac{1}{45} \cdot \frac{m+n+1}{(m+n)^2} \cdot \frac{4mn(m+n) - 3(m^2 + n^2)-2mn}{4mn}.
\end{equation}
We normalize the $T_{m,n}$ from \ref{eq:CMstat}
\begin{equation}
T_{m,n}^\mathrm{norm} = \frac{T_{m,n}-\mathrm{E}[T_{m,n}]}{\sqrt{45\mathrm{Var}[T_{m,n}]}} + \frac{1}{6},
\end{equation}
so we can compare it with the values of limiting distribution $z$ tabulated in \cite{AndersonDarling1952}. We find the desired $p-$value $p_T$ by interpolation of $z$ at the point $T_{m,n}^\mathrm{norm}$. If $\alpha < p_T$ we reject the null hypothesis that $F = G $ with significance level $\alpha$.

Again for use with the weighted samples which we need to analyze in case of \dzero data we replace the ECDFs in the statistic $T_{m,n}$ by appropriate WECDFs.

\section{\ren Divergence Based Ranks}
\ren divergence has proved in many fields of information theory as a very robust distance. 

Even though there haven't been devised any specific test to go with \ren divergence, we used it to rank the variables, so we had another measure to confirm our results. Moreover, we wanted not only to test similarity of MC and data, we also wanted to now the difference between signal and background channels and there are not tests for that purpose in the literature.

%\ren divergence is a $\phi-$divergence 
%\begin{equation}
%\mathfrak{D}_\phi (P,Q) = \int q \,\phi\left( \frac{p}{q} \right)\,\mathrm{d}\mu
%\end{equation}
%with the choice 
%\begin{equation}
%\phi =  ,
%\end{equation}
%so the final form is 
%\begin{equation}
%\mathfrak{R}_\alpha = \frac{1}{a(a-1)} \int p^\alpha q^{1-\alpha}
%\end{equation}

