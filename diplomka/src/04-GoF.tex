\chapter{Goodness of Fit Tests}
\label{ch:GoF}

signal vs background \\
neco o porovnani teoretickeho a root KS \\
graf histogram-bins(n)\\ 2000 - 10000
\\
\\


\noindent The Goal of this chapter is to develop and test several tools for ranking and comparing variables and selecting the suitable ones for consequential analyses. There are number of reasons for decreasing the dimension of the problem at hand. 

One of them is so called {\em curse of dimensionality}. It stands for the phenomena which occurs in high-dimensional spaces. The problem is that with increasing dimension the volume of the analysed space increases so fast that the available data become sparse. Therefore any result of the analyses is statistically insignificant due to insufficient amount of data. Another, maybe less significant, reason is of course amount of time and resources needed to complete the computations relevant to the analysis. 

There are two criteria which we used for comparing the variables. One of them is the similarity between DATA and MC. The similarity is needed, since most of the methods used for separation use the MC for training. That is process of associating specific features found in the data with required result of the method. The second criterion is the separating strength of the variable. Therefore in both criteria we measure distance between distributions of different data sets. The similarity criterion implies the need of small distances which leads to goodness of fit (GoF) \nomenclature{GoF}{Goodness of fit} tests . The second criterion on the other hand requires the distance to be as large as possible. 

There are number of two-sample goodness of fit  tests, e.g. Kolmogorov-Smirnov test, Cram\'{e}r-von Mises test and many others. There is one requirement we have to meet by the choice of the test. Because we have weighted data samples, we have to incorporate the data weights to the method. This can be done by the use of WECDF  \eqref{eq:wecdf} which is the basis of the statistics of aforementioned tests. 

We consider two i.i.d. weighted random samples $\mathbf{X}^F = (X^F_1,\ldots, X^F_m)$ and $\mathbf{X}^G = (X^G_1, \ldots, X^G_n)$ with respective weights 

\begin{align*}
\mathbf{w}^F = (w^F_1,\ldots, w^F_m), \quad w_i^F \in \mathbb{R}^+ \text{ for }  i \in \lbrace 1, \ldots, m \rbrace, \\
\mathbf{w}^G = (w^G_1,\ldots, w^G_n), \:
\quad w_i^G \in \mathbb{R}^+ \text{ for } i \in \lbrace 1, \ldots, n \rbrace. 
\end{align*}
%generated by probability distributions $F$ and $G$ respectively. From these two samples, we construct ECDFs $F_m(x),\, G_n(x)$  and WECDFs  $F^w_m(x),\, G^w_n(x).$

\noindent By $F_m(x) $ and $ G_n(x)$ we  denote empirical distribution functions of samples $\mathbf{X}^F$ and $\mathbf{X}^G$, respectively, and by $F^w_m(x)$ and $ G^w_n(x)$ we denote weighted empirical distribution functions corresponding to these samples. We also define so called joint sample 
\begin{equation}
\mathbf{X}^{F\cup G} = (X^{F\cup G}_1, \ldots, X^{F\cup G}_{m+n}) = (X^{F}_1, \ldots, X^{F}_{m},X^G_1,\ldots, X^G_n) = \mathbf{X}^{F} \cup \mathbf{X}^{G}
\end{equation}
with weights 
\begin{equation}
\mathbf{w}^{F\cup G} = (w^{F\cup G}_1,\ldots, w^{F\cup G}_{m+n}) = (w^{F}_1,\ldots, w^{F}_{m},w^{G}_1,\ldots, w^{G}_{n}) = \mathbf{w}^{F} \cup \mathbf{w}^{G}
\end{equation}
and with its ECDF $H_{m+n}(x)$ and WECDF $H^w_{m+n}(x)$.

% First we are going to list tests and ranks which we used for analysing the data.

\noindent The two-sample problem is represented by the test
\begin{equation}
H_0: F = G \quad\: \text{vs.}\quad H_1: F \neq G \quad\text{ at a given significance level } \alpha.
\end{equation} 
Since we have to take into account the weights associated with dataset, the most convenient way is to use tests based on ECDFs where we can replace standard c.d.f. by WECDFs. 

Most tests (including those mentioned below) are devised for relatively small sample sizes $m,n$. There usually exists some exact theoretical distribution for sample sizes below 10. Sample sizes around 20 and more are often enough to use appropriate asymptotic distribution. Because we are dealing with the sample sizes $m,n \geq 2000$, we are quite confident of using these asymptotic distributions.

\section{Kolmogorov-Smirnov Test}
The Kolmogorov-Smirnov (shortly KS) \nomenclature{KS}{Kolmogorov-Smirnov} goodness of fit test is based on Glivenko-Cantelli theorem \ref{theo:glivenko-cantelli}. Statistic $D_{m,n}$, defined as Kolmogorov distance between ECDFs of analysed samples
\begin{equation}
D_{m,n} = \sup_{x \in \mathbb{R}} |F_m(x) - G_n(x)|,
\label{eq:KSstat}
\end{equation}
is used. If $H_0$ is true, then according to Theorem \ref{theo:glivenko-cantelli} it holds that $D_{m,n} \rightarrow 0$ almost surely for $m \rightarrow \infty$ and $n \rightarrow \infty$. Now we present theorem which servers for constructing the exact KS test.
\begin{theorem}[Smirnov]
	Let us denote  
	\begin{equation}
	K(\lambda) = 1 - 2\sum_{k=1}^{\infty} (-1)^{k+1} \exp\left[ -2k^2\lambda^2\right].
	\end{equation}
	Then 
	\begin{equation}
	\lim_{m,n \rightarrow \infty} \mathrm{P}\left(\sqrt{\frac{mn}{m+n}}D_{m,n} < \lambda \right) = K(\lambda)
	\end{equation}
	holds for all $\lambda$.
\end{theorem}
\noindent Proof can be found in \cite{Smirnov1944}. 

\noindent Kolmogorov Smirnov test computes the specific value
\[\lambda_0 = \sqrt{\frac{mn}{m+n}}D_{m,n}\] and consequently we evaluate  $K\left( \lambda_0 \right)$. If $K\left( \lambda_0 \right) \geq 1 - \alpha$, we reject hypothesis $H_0$ at significance level $\alpha$.

As stated above, we need to reweight the MC entries so they match the data events. Therefore we have to replace the ECDFs in the statistic $D_{m,n}$ by appropriate WECDFs
\begin{equation}
D_{m,n} = \sup_{x \in \mathbb{R}} |F^w_m(x) - G^w_n(x)|.
\end{equation}



\section{Cram\'{e}r-von Mises test}
Two sample Cram\'{e}r-von Mises (shortly CM) \nomenclature{CM}{Cram\'er-von Mises} goodness of fit test is similar to aforementioned KS test. Instead of Kolmogorov distance, CM test  is based on the $\omega^2$ criterion,
\begin{equation}
\omega^2 = \frac{mn}{m+n} \int_{-\infty}^\infty \left[F_m(x) - G_n(x) \right]^2 \,\mathrm{d} H_{m+n}(x).
\end{equation} 
Statistic derived from this integral (described in \cite{Anderson1962}) is
\begin{align}
T_{m,n} & = \frac{mn}{(m+n)^2}\left( \sum_{i=1}^m \left( F_m\left(X^F_i\right) - G_n\left(X^F_i\right)\right)^2 + \sum_{j=1}^n \left( F_m\left(X^G_j\right) - G_n\left(X^G_j\right)\right)^2 \right) \\
& = \frac{mn}{(m+n)^2} \sum_{i=1}^{m+n} \left( F_m\left(X^{F\cup G}_i\right) - G_n\left(X^{F\cup G}_i\right)\right)^2 .
\label{eq:CMstat} %\\
%& = \frac{mn}{m+n} \sum_{i=1}^{m+n} \frac{1}{m+n} \left( F_m\left(X^{F\cup G}_i\right) - G_n\left(X^{F\cup G}_i\right)\right)^2. 
\end{align}
%Here 1/(m+n) in the sum is weight equal for all data and it is the amount added to the ECDF at each data point. Because we don't have equally weighted data, we have to replace the fraction by weight ${w_i^{F\cup G}}$ specific for each data point. We can write the weighted statistic as
%\begin{equation}
%T^w = \frac{mn}{(m+n)} \sum_{i=1}^{m+n} w_i^{F\cup G} \left( F_m(X^{F\cup G}_i) - G_n(X^{F\cup G}_i)\right)^2. 
%\end{equation}
The expected value $\mathrm{E} [T_{m,n}]$ and variance $\mathrm{Var} [T_{m,n}]$ are %derived in  and  
under the null hypothesis 
\begin{equation}
\mathrm{E} [T_{m,n}] = \frac{1}{6} + \frac{1}{6(m+n)},
\end{equation}
and
\begin{equation}
\mathrm{Var} [T_{m,n}] = \frac{1}{45} \cdot \frac{m+n+1}{(m+n)^2} \cdot \frac{4mn(m+n) - 3(m^2 + n^2)-2mn}{4mn},
\end{equation}
see \cite{Anderson1962} for derivation. We normalize $T_{m,n}$ from \eqref{eq:CMstat} to
\begin{equation}
T_{m,n}^\mathrm{norm} = \frac{T_{m,n}-\mathrm{E}[T_{m,n}]}{\sqrt{45\mathrm{Var}[T_{m,n}]}} + \frac{1}{6}.
\label{eq:CMstat-lim}
\end{equation}
It has been shown in \cite{Rosenblatt1952} that for $m \rightarrow \infty$, $n \rightarrow \infty$, and for ${m}/{n} \rightarrow \lambda$, where $\lambda$ is positive finite constant, the  $T_{m,n}^\mathrm{norm}$ has the same asymptotic distribution $z$ as the one-sample variant of the test. The distribution $z$ is tabulated  in \cite{AndersonDarling1952}. We find the desired $p-$value $p_T$ by interpolation of $z$ at the point $T_{m,n}^\mathrm{norm}$. If $\alpha < p_T$ we reject the null hypothesis that $F = G $ at significance level $\alpha$.

Again for the reweighted samples, which we need to analyze in the case of \dzero data we replace the ECDFs in the statistic $T_{m,n}$ by appropriate WECDFs.

\section{Anderson-Darling test}
Anderson-Darling (shortly AD) \nomenclature{AD}{Anderson-Darling} two-sample goodness of fit test is essentially a generalization of the CM test. Generalized CM statistic can be written as 
\begin{equation}
\omega^2 = \frac{mn}{m+n} \int_{-\infty}^\infty w(x) \left[F_m(x) - G_n(x) \right]^2 \,\mathrm{d} H_{m+n}(x),
\end{equation}
where $w(x)$ is a weight function adjusting the importance of certain intervals of the sample space. It is obvious, that CM is a special case of AD test with the choice $w(x) = 1 \:\: \text{ for all } x \in \mathbb{R}.$ Anderson and Darling   in \cite{AndersonDarling1954} introduced the statistic 
\begin{equation}
A^2_{N} = \frac{mn}{N} \int_{-\infty}^\infty \frac{\left[F_m(x) - G_n(x) \right]^2}{H_{N}(x)\left[ 1 - H_{N}(x)\right]} \,\mathrm{d} H_{N}(x),
\label{eq:ADstat}
\end{equation}
where $N = m+n$. The integrand is set to be zero if $H_{N}(x) = 1$. For our purposes, we again replace the ECDFs $F_m$ and $G_n$ by their weighted versions $F_m^w$ and $G_n^w$, respectively.
According to Pettitt \cite{Pettitt1976} we reject hypothesis $H_0$ at significance level $\alpha$ whenever
\begin{equation}
\frac{A^2_{kN} - (k-1)}{\sigma_{N}} \geq z_{k-1}(1-\alpha).
\end{equation}
Here $k$ stands for the number of the compared samples, therefore we use $k=2$. By $z_{k-1}(1-\alpha)$ we denote the $(1-\alpha)$-quantile of the standardized asymptotic distribution $Z_{k-1} = [A^2_{k-1} - (k-1)]/{\sigma}$, which is tabulated by Sholz and Stephens in \cite{Sholz1986} for typical values of $\alpha$.  Because the test is not restricted to two samples, they present values for up to ten compared samples.
For standardization of $A^2_{N}$ we can use the results from \cite{Sholz1986} 
\begin{equation}
\sigma_{N}^2 = \mathrm{Var}\left[ A^2_{N}\right] = \frac{ aN^3 + bN^2 + cN + d
 }{(N-1)(N-2)(N-3)},
\end{equation}
with 
\begin{align*}
a = & (4g - 6)k + (10 - 6g)H - 4g + 6, \\
b = &(2g - 4)k 2 + 8hk + (2g - 14h - 4)H - 8h + 4g - 6, \\
c = &(6h + 2g - 2)k2 + (4h - 4g + 6)k + (2h - 6)H + 4h,  \\
d = &(2h + 6)k 2 - 4hk, 
\end{align*}
where
\begin{equation}
H = \frac{1}{m}+\frac{1}{n},\quad	h = \sum_{i=1}^{N-1}\frac{1}{i},\quad \mathrm{ and } \quad g = \sum_{i=1}^{N-2}\sum_{j=i+1}^{N-1}\frac{1}{(N-i)j}\rightarrow \frac{\pi^2}{6}.
\end{equation}
%Note that
%\begin{equation}
%g \rightarrow \int_0^1 \int_y^1 \frac{1}{x(1-y)}\,\mathrm{d}x \,\mathrm{d}y = \frac{\pi^2}{6}.
%\end{equation}

\section{\ren Divergence Based Ranks}
Although there has not been devised any specific test applying the \ren divergence (RD) \nomenclature{RD}{R\'enyi divergence} defined by \eqref{eq:renDiv}, we use RD as a statistic to compare the variables and rank them on the basis of this measure. % We do so to have another measure to compare with our results. Also, there are two tasks for 
Moreover, apart from testing similarity of MC and data, we also want to know the difference between signal and background channels. There are not any tests for that purposes in the literature and therefore any statistic can be used as a relative measure. 

Advantage of previous tests over RD is the use weighted empirical distribution functions. For using RD, we have to compute some estimator of p.d.f. In this work, we used equidistant histograms with various numbers of bins and kernel density estimator. Histogram is nonparametric estimator describing data through relative frequency in disjoint intervals called bins. Closed interval $[a,b]$ is divided to $m$ equidistant intervals defined by points $\lbrace t_0, \ldots, t_m \rbrace$, where $a = t_0 < \cdots < t_m = b.$ Histogram is then defined as 
\begin{equation}
\hat{f}_n(x) = \frac{1}{n} \sum_{i=1}^m \frac{N_i}{t_i - t_{i-1}} I_{(t_i, t_{i-1}]}(x),
\end{equation}
where $N_i$ is number of observations in interval $(t_i, t_{i-1}]$. 

Histograms are known to have issues with stability regarding varying number of bins and  the choice of the interval $[a,b]$.

\begin{table}[ht]
\centering
\begin{tabular}{|l | c|}
\hline 
\rule{0pt}{2.4ex} Square root & {$ m = \sqrt{n}$} \\% [1.1ex]
\hline 
\rule{0pt}{2.4ex} Sturge's formula \cite{Sturges1926} &$ m = \lceil\log_2 n + 1\rceil$ \\ %[1.1ex]
\hline
\rule{0pt}{2.4ex} Rice rule \cite{RiceRule}&  $m = \lceil 2 \sqrt[3]{n}\rceil$ \\ %[1.1ex]
\hline
\rule{0pt}{2.4ex} Doane's formula \cite{Doane1976} &  $m =  m_{\mathrm{Doane}}(n)$ \\
\hline
\rule{0pt}{4ex} Scott \cite{Scott1979} &  $m = \dfrac{3.49 \hat{\sigma}}{\sqrt[3]{n}}$ \\
\hline
\end{tabular} 
\caption{Formulas for computing number of bins in equidistant histograms, where $n$ is the sample size, $m$ is the desired number of bins.}
\label{tab:histNbin}
\end{table}

\noindent Functions for computing the number of bins are shown in Table \ref{tab:histNbin}, where \[ m_{\mathrm{Doane}}(n) = 1 + \log_2 (n) + \log_2 \left( 1 + \dfrac{|g_1|}{\sigma_{g_1}} \right), \quad   \text{ where } \sigma_{g_1} = \frac{6(n-2)}{(n+1)(n+3)}\] and $g_1$ is the estimated skewness of the dataset. 

