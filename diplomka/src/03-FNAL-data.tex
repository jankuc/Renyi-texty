\chapter{Fermilab Data and Separation Strategy}\label{ch:FNAL_data}

In this chapter we present data which was analyzed in this work. They were obtained from the experiment \dzero in Fermilab. In the following, we deal with different data sets because the data originate from different physical processes. Some of them differ in significant physical properties and thus our data processing and analyses also vary. By every mentioned set we actually mean two separate sets. The first one contains real data (simply DATA\nomenclature{DATA}{Real measured data from experiment \dzero}) measured by the detector \dzero. The second set contains Monte Carlo simulations (so called Monte Carlo or shortly MC\nomenclature{MC}{Monte Carlo simulated data}), which is generated so that it corresponds to the measured data. Furthermore MC is divided into three more separate sets: {\em Training sample, Testing sample} and {\em Yield sample}. 

\begin{itemize}
\item {\em Training sample} is used for training supervised and unsupervised separation methods. 
%
\item {\em Testing sample} can be also used for training of the unsupervised methods and for the testing phase of supervised methods. That is the part, when the supervisor (teacher) tells the separation algorithm how well it classifies some unknown data (data which were not used for learning). Separation methods usually use this testing sample to prevent overfitting which can occur when the classifier starts to memorize the training data rather then learning to generalize it.
%
\item {\em Yield sample} is used for checking how the method performs on completely unknown data. It is used for "dry runs", before the classifier is used to separate actual DATA. This set is usually reweighted so it has the same volume as the DATA set.
\end{itemize}

In order to cover all the possible effects, there are many more entries in the MC. Because of that, all the entries in MC are reweighted so that the volume of the weighted MC set corresponds to the volume of DATA. These reweighted entries (and data entries) are then called events.

\section{\texorpdfstring{\dzero}{D0} Experiment at Tevatron}
\dzero was one of the biggest experiments at Tevatron. Discovery of top quark was the main focus of the \dzero experiment on this synchrotron. Top quark was predicted by Standard model and discovered in Tevatron in 1995. Its mass is the biggest one of all the subatomic particles and it is subject to continual changes due to ongoing analyses. The latest value is $m_\mathrm{top} = 173.34 \pm 0.76 \,\mathrm{GeV/c}^2 $, which comes from international collaboration of CERN's experiments ATLAS, CMS at the synchrotron LHC and of Fermilab's CDF, \dzero at Tevatron \cite{jointMass}. Illustration of this result is shown in Figure \ref{fig:jointMass}. 

\begin{figure}[h]
	\centering
	\includegraphics[width=0.6\textwidth]{Top-Mass-graphic-hr.jpg}
	\caption{World collaboration arrives at the new best value for top quark mass.}
	\label{fig:jointMass}
\end{figure}

The data samples analyzed here were collected at \dzero experiment in the run period called Run II, which started in July 2002 and ended in September 2011 \cite{Yuntse}.

\section{Top Quark} \label{sec:topQuark}
Top quark is produced mostly in pairs top-antitop quark (written as $t\bar{t}$). In 2009 the collision from which single top quark originated  (without its antiparticle) was detected. Due to its high mass, the top quark (and antitop quark) decays in very short time, even before forming hadrons. It happens through weak interaction when top quark almost exclusively decays to $W-$boson and $b-$quark as can be seen in Figure \subref*{fig:feynman-ljets}. $W-$boson then decays in three different ways: it can form one of the leptons (electron, muon or tauon) and jets, two leptons or just jets. Figure \subref*{fig:ttbarBranchingFrac} shows probability of these events. 

Another parameter which changes physical properties of the collision is the number of jets present in the decay of $W-$boson. We recognize three categories: $2$ jets, $3$ jets and more than $4$ jets (shortly denoted by $4+$ jets).

\begin{figure}[thb]
  \centering
  \subfloat[t][Top pair branching fractions]{%
  	\includegraphics[width=.55\textwidth]{top_pair_branching_frac.eps}%
  	\label{fig:ttbarBranchingFrac}%
  }%
  \subfloat[t][Feynman diagram of top pair decay to lepton+jets: 
    $q - \text{quark}$,
    $\bar{q} - \text{antiquark}$,
	$g - \text{gluon}$,
	$t - \text{top quark}$,
    $W - \text{W--boson}$,
    $l - \text{lepton}$,
    $\nu - \text{neutrino}$,
    $b - \text{bottom quark}$ \cite{Heinson}]{%
  	\includegraphics[width=.4\textwidth]{feynman_ttbar_ljets.png}%
  	\label{fig:feynman-ljets}%
  }
  \caption{Decay of \ttbar pair}
\end{figure} 

All data are filtered many times due to many reasons. First preprocessing takes  place in the process of collecting data events, because the data flow was simply too large to store in time. The frequency of collisions recorded by \dzero was around 1.7 MHz, which was reduced by 3 levels of triggers to 150 Hz for storing on tapes \cite{Yuntse}. Other filtering processes were carried out in consideration to the following analysis. One of the event filters is called $b-$tagging, i.e. picking those events in which $b-$quarks appeared (with specific degree of certainty) as one of the results of top quark decay. This criterion ensures greater percentage of the required signals in the dataset. According to this criterion, three groups are distinguished: events before tagging, events with one $b-$tag (1b) and two $b-$tags (2b).

Our goal is to find separation criteria for the electron + jets and muon + jets channels before $b-$tagging. As stated above, we recognize subsets with $2, \,3,\, 4+$ jets for both channels. This means that we have six datasets with slightly different physical characteristics and therefore we have to analyse them separately. 

\section{Variables}\label{sec:Variables}

\dzero detector  produces high dimensional data (number of physical parameters  goes as high as $600$ items). Since a lot of these parameters are strongly correlated, or incorrectly modeled by the Monte-Carlo simulations (in the scope of needed accuracy), there are steps to decrease the dimension by transforming the data, leaving out selected variables and so on. This process is again analysis-dependent. Data used in this work contain 24 variables of which not all are useful or even defined in all datasets. For example, since the variable called \textsf{HT3} is the scalar sum of the transverse momentum of the three leading jets, it is not defined for lepton + 2 jets decay channel. Furthermore, the variable called \textsf{Lepemv} is the output of electron discriminant, which is probability that the lepton in the decay is electron. Therefore it is not defined for the muon channel. Number of variables depending on number of jets and type of lepton are listed in Table \ref{tab:data-dims}.

\begin{table}[htb]
\caption{Dimension of datasets with respect to number of jets and observed lepton.}
\centering
\begin{tabular}{|l|c|c|}
\hline 
 & electron & muon \\ 
\hline 
2 jets & 23 & 22 \\ 
\hline 
3 jets & 24 & 23 \\ 
\hline 
4+ jets & 24 & 23 \\ 
\hline 
\end{tabular} 
\label{tab:data-dims}
\end{table}

Variables are listed and briefly explained in Table \ref{tab:vars}. This selection comes from  FNAL personnel and contains these 24 variables, representing good enough separation strength, good similarity of MC and DATA and they are in some sense complementary to the variables used by previous \dzero analyses. This is also required for the possibility of combination of analyses which benefits more from uncorrelated variables than it does from correlated. 

Former \dzero analyses were using only the first 7 variables: \textsf{Apla, Spher, HTL, JetMt, HT3, MEvent, MT1NL}. One of our goals is to argue if there are variables with better separation strength or with some other better properties. 

\section{Signal and Backgrounds}
\input{src/fig-9vars.tex}

The final purpose of the analyses is to separate the certain required signal from the background events comprising of other collisions, other decays and so on. Our dataset consists of 2 signals (\textsf{ttA\_172, ttAll\_172}) and 15 backgrounds. Their list can be found in Tables \ref{tab:chnl-ele} and \ref{tab:chnl-muo}, where we can also find their proportional representations in the MC depending on the lepton and number of jets. Columns next to the percentages are number of entries (before re-weighting). Percentages in the lower part of the tables are the ratios of MC to DATA and DATA to MC, respectively.

\input{tables/tab-vars.tex}

\input{tables/tab-channels-muo.tex}

Proportional representations of the signal and backgrounds in MC can be also shown via histograms, so called control plots, e.g. at the top part of Figures  \ref{fig:contrPlots1} and \ref{fig:contrPlots2}. The lower part shows the similarity of MC and DATA via the ratio
\begin{equation*}
\frac{number\:of\:data\:in\:the\:bin - weights\:of\:MC\:in\:the\:bin}{number\:of\:data\:in\:the\:bin}.
\end{equation*}
Script generating these figures can be found in \texttt{git} version control directory \texttt{root\_scripts}, see chapter \ref{ch:software} for details. 

\begin{figure}[thb]
  \centering
  \subfloat[t][Ht20, 3 jets, electron ]{%
  	\includegraphics[width=.65\textheight]{samples_plots-png/yield/ele/3jet/ele_3jet_Ht20.png}%
  	\label{fig:contr-ele}%
  }%
  \\
  \subfloat[t][Ht20, 4 jets, electron]{%
  	\includegraphics[width=.65\textheight]{samples_plots-png/yield/ele/4jet/ele_4jet_Ht20.png}%
  	\label{fig:contr-muo}%
  }
  \caption{Control plots of the proportional representation of signal and backgrounds in the MC compared to DATA.}
  \label{fig:contrPlots1}
\end{figure} 

\begin{figure}[thb]
  \centering
  \subfloat[t][Lepdphimet, 2 jets, muon ]{%
  	\includegraphics[width=.65\textheight]{samples_plots-png/yield/muo/2jet/muo_2jet_Lepdphimet.png}%
  	\label{fig:contr-ele}%
  }%
  \\
  \subfloat[t][Centr, 4 jets, muon]{%
  	\includegraphics[width=.65\textheight]{samples_plots-png/yield/muo/4jet/muo_4jet_Centr.png}%
  	\label{fig:contr-muo}%
  }
  \caption{Control plots of the proportional representation of signal and backgrounds in the MC compared to DATA.}
  \label{fig:contrPlots2}
\end{figure} 

Figure \ref{fig:hists-9fig} depicts the histogram of signal and background for 3 selected variables in 2, 3 and 4+ jets. We can see that the signal-to-background ratio increases with increasing number of jets. This phenomenon can also be observed in Tables \ref{tab:chnl-ele} and \ref{tab:chnl-muo}. 

\clearpage

\section{Divergence Trees}\label{s:DDT}

Divergence decision tree (DDT) \nomenclature{DDT}{Divergence decision tree} is a relatively new method of unsupervised classification. It was presented in 2005 in \cite{Karakos2005}. Our algorithm is inspired by \cite{Karakos2008} and it is a type of Integrating Sensing and Processing Decision Trees (ISPDTs). It  begins with the whole data sample at the root node. The growing algorithm is as follows. The data in each node can be transformed into a lower dimensional space and divided into two clusters according to an optimization criterion via arbitrary classification method. Each node therefore represents a subset of the data of its parent node.  There is a possibility of adding a supervisor in this step. If we have training data, the transformation and clustering may be optimized to maximize the separation criterion between the classes. The separation criterion can be, for example, the maximization of the minimum distance between data points in different clusters or the maximization of the distance between clusters. 

Stopping criterion is used to mark the nodes that should not be divided further, therefore they become leaves. This criterion can be based on some level of purity of the node. Node becomes "pure" when all the data in the node belongs to one class. Another criterion could be some minimal amount of data in the node. When one of the defined stopping criteria is met, the node becomes leave and is not eligible for splitting any more. 

Let $\mathcal{C}$ be a collection of data points $\mathbf{X}_1,\ldots,\mathbf{X}_n \in \mathcal{X}^d.$

\begin{definition}
The \ren divergence $\mathfrak{R}_\alpha(S)$ of the subset $S\subset \mathcal{C}$ of data points is defined by
\begin{equation}
\mathfrak{R}_\alpha(S) = \min_Q \sum_{\mathbf{X} \in S} \mathfrak{R}_ \alpha(P_\mathbf{X}, Q),
\end{equation}
where $P_\mathbf{X}$ is the empirical marginal distribution function derived from $\mathbf{X}$ and $Q$ is a distribution function on $\mathcal{X}.$ 
\end{definition}

\noindent $\mathfrak{R}_\alpha(S)$ provides an indication of homogeneity of set $S$. For example $\mathfrak{R}_\alpha(S) = 0$ means that all elements of $S$ have the same distribution. Therefore we look for a partition $A_1,\ldots,A_m$ of $\mathcal{C}$ with the smallest possible $m$, such that
\begin{equation}
\sum_{i=1}^m \mathfrak{R}_\alpha(A_i) < \varepsilon \qquad \text{ for some } \varepsilon > 0.
\end{equation}
Without the requirement of smallest possible $m$, we would always arrive to the partition $A_1,\ldots,A_n$, where $A_i = \lbrace X_i\rbrace$ for all $i=1,\ldots,n.$ 

Thus, the algorithm for growing the tree is as follows:\\

Find node $S$ with the highest $\mathfrak{R}_\alpha(S)$
\begin{enumerate}
\item if $\mathfrak{R}_\alpha(S) < \varepsilon$, declare $S$ to be  pure enough
\item if $|S| \leq n_\mathrm{min}$, declare $S$ to be pure
\item otherwise split the node into two subsets $A_1, A_2 $ such that $\mathfrak{R}_\alpha(A_1) + \mathfrak{R}_\alpha(A_2)$ is as small as possible \label{step}
\end{enumerate}


\noindent The tree iterations continue until all created leaves meet one of the stopping criteria or a desired number $m$ of nodes has been reached. 

It is clear that the splitting in the step \ref{step} can be done via large number of already used classification methods. We used some basic unsupervised clustering techniques like $k$-means and a few of their more advanced modifications like  \emph{DBSCAN} \cite{Ester} or its more advanced version \emph{SUBCLU} \cite{Kailing}. These methods  participated very poorly on the high dimensional FNAL datasets due to the lack of supervision. Further tests are therefore in progress using the \emph{Support Vector Machine} supervised learning model and also the \emph{Model Based Clustering} is going to be implemented. 






