\chapter*{Introduction}

\chapter{Minimum Distance  Estimation}
\section{MDE definitions}
In this section we consider a parametric family $\mathcal{F} = \lbrace F_\theta : \theta \in \Theta \subset \mathbb{R}^m \rbrace$, of distribution functions on $\mathbb{R}$, where $\Theta \subset \mathbb{R}^m$ is metric space with Euclidean distance 
\begin{equation}
	\rho (\theta_1,\theta_2) = \sqrt{(\theta_1-\theta_2)(\theta_1-\theta_2)^T}, \quad \theta_1, \, \theta_2 \in \Theta.
\end{equation}
Furthermore, we assume identifiability of the family $\mathcal{F}$, which means 
\begin{equation}
\theta_1 \neq \theta_2 \Rightarrow F_{\theta_1} \neq F_{\theta_2}
\end{equation}
and also that all of $F_\theta(x), \, x \in \mathbb{R}$ are measurable. Our estimators will be calculated on the basis of i.i.d. observations $\mathbf{X}_n = (X_1, \ldots ,X_n)$ all distributed by distribution function $F_{\theta_0} \in \mathcal{F}$. Then $F_{\theta_0} \in \mathcal{F}$ is a true distribution function and $\theta_0 \in \Theta$ is a true parameter.

We are interested in non-parametric estimates of the unknown distribution $F_{\theta_0}$ based on $\mathbf{X}_n$, which are sequences of mappings $ \mathbb{R}^n \rightarrow \mathcal{F}(\mathbb{R}) $ or directly in estimates of the unknown parameter $\theta_0$ (point estimator) based on the same sample $\mathbf{X}_n$, which is sequence of functions $\\hat{\theta}_n : \mathbb{R}^n \rightarrow \Theta$. If $\mathcal{F}$ is dominated by a $\sigma$-finite measure $\lambda$ and we write \RN density as
\begin{equation}
f_\theta = \dfrac{\mathrm{d} F_\theta}{\mathrm{d} \lambda},
\end{equation}
we might be also interested in estimator $\hat{f}_n : \mathbb{R}^n \rightarrow \lbrace f_\theta : \theta \in \Theta  \rbrace$ of unknown density $f_{\theta_0}$.

The error of point estimates $\hat{\theta}_n$ is evaluated by $\rho (\hat{\theta}_n,\theta_0)$, error of density estimates by  $\| f_{\hat{\theta}_n} - f_{\theta_0} \|$, where
\begin{equation}
\| f_{\theta_1} - f_{\theta_2} \| = \int{|f_{\theta_1} - f_{\theta_2}|\,\mathrm{d}\lambda}
\end{equation}
is $\mathrm{L}_1$-norm. For evaluating the deviation of distribution functions we need some distance $\mathfrak{D}(F,G) $ on $\mathcal{F}(\mathbb{R}).$

\begin{definition} % 1 
For a set of random variables $X_n$ the notation $X_n = o_p(1)$ means that it holds $\lim_{n \rightarrow \infty } X_n = 0$ in probability. The notation $X_n = o_p(\varepsilon_n)$ means that $\frac{X_n}{\varepsilon_n} = o_p(1)$ for some sequence $\varepsilon_n \searrow 0$.
\end{definition} 

\begin{definition} % 2
We say that an estimator $\hat{F}_n$ is consistent estimator of $F_{\theta_0}$ in distance $\mathfrak{D}$, if $\mathfrak{D}(\hat{F}_n, F_{\theta_0}) = o_p (1)$ and is measurable in $\mathbf{X}_n$.
\end{definition}

%\begin{definition}
%Řekneme, že posloupnost náhodných veličin $\lbrace X_n \rbrace$ s distribučními funkcemi $\lbrace F_n \rbrace$ je omezená v pravděpodobnosti, pokud pro každé $\varepsilon > 0 $ existují $M$ a $N$ takové, že 
%	\begin{equation}
%		F_n(M) - F_n(-M) > 1-\varepsilon, \quad \forall n > N.
%	\end{equation}
%	Tuto skutečnost značíme $X_n = O_p(1)$.
%\end{definition} 

As an example of non-parametric estimator of distribution function we can use empirical distribution function defined as 
\begin{equation}
F_n(x)=\frac{1}{n}\sum_{j=1}^n \textbf{I}_{(-\infty,x]}(X_j), \qquad 
x\in\mathbb{R}.
\end{equation}

%
%\begin{theorem}[Glivenko-Cantelli]
%Nech\v{t} jsou $X_1,\dots,X_n$ stejně a nezávisle rozdělené reálné ná\-ho\-dné veličiny s distribuční funkcí $F$ a nechť $F_n(x)$ je empirická distribuční funkce. Pak pro každé $n\in \mathbb{N}$ a $\varepsilon>0$ platí
%
%\begin{equation}
%P\left(\sup_{x\in\mathbb{R}}|F(x)-F_n(x)|>\varepsilon\right) \leq 8(n+1)\exp{\left\{ \frac{-n{\varepsilon}^2}{32}\right\}}.
%\end{equation}
%Z Borel-Cantelliho lemma pak plyne
%\begin{equation}
%\lim_{n \rightarrow +\infty}\sup_{x \in \mathbb{R}}|F(x)-F_n(x)|=0  \quad \text{s. j.}
%\end{equation}
%\end{theorem}

Now we can define minimum distance estimator (MDE) and asymptotically minimum distance estimator (AMDE).

\begin{definition}\label{def-mde}
	Let $\mathfrak{D}(\cdot, \cdot) $ be distance on $\mathcal{F}$ and $\hat{F}_n$ an estimator of distribution function $F_{\theta_0}$. A point estimator $\hat{\theta}_n$ measurable in $\Theta$ satisfying the condition
	\begin{equation}
		\mathfrak{D}(\hat{F}_n, F_{\hat{\theta}_n}) = \inf_{\theta \in \Theta}(\hat{F}_n, F_{\theta})
		\label{eq:MDE}
	\end{equation}
	is said to be minimum distance estimator (MDE). If the family $\mathcal{F}$ is dominated by some $\sigma$-finite measure $ \lambda$, then estimator's \RN density $f_{\hat{\theta}_n}$ is said to be minimum distance probability density estimator. If the estimator meets at least condition
	\begin{equation}
		\mathfrak{D}(F_n, F_{\hat{\theta}_n}) - \inf_{\theta \in \Theta}(F_n, F_{\theta_0}) = o_p(n^{-\frac{1}{2}}), 
	\end{equation}
	 instead of condition \ref{eq:MDE}, the function $\hat{\theta}_n$ is said to be an asymptotic minimum distance estimator (AMDE) and the corresponding probability density function $f_{\hat{\theta}_n}$ is said to be an asymptotic minimum distance probability density estimator.
\end{definition}

Note that the rate $n^{-\frac{1}{2}}$ is not important and it can be replaced by any $\varepsilon_n$ tending to 0 sufficiently fast.

\subsection{Decomposable pseudodistances}

In this section we will look into \ren pseudodistance. 
		
		\begin{definition}
	We say that the mapping $\mathfrak{D}:\mathcal{P}\times\mathcal{P}^+ \rightarrow \mathbb{R}$ is a pseudo-distance between probability measures $P \in \mathcal{P}$ and $Q \in \mathcal{P}^+$ if it holds		
		\begin{equation}
			\mathfrak{D}(P_\theta,Q) \geq 0 \quad \textit{for all } \theta \in \Theta \quad \text{and} \quad Q \in \mathcal{P}^+
		\end{equation}
		and 		
		\begin{equation}
			\mathfrak{D}(P_\theta,P_{\tilde{\theta}})=0 \; \Leftrightarrow \; \theta=\tilde{\theta}.
		\end{equation}	
	This pseudo-distance is decomposable if there exist functionals such that
		 $\mathfrak{D}^0:\mathcal{P}\rightarrow\mathbb{R}$, $ \mathfrak{D}^1:\mathcal{P}^+ \rightarrow \mathbb{R}$ and measurable mapping
		  $\rho_\theta : \mathcal{X} \rightarrow \mathbb{R}$, $ \theta \in \Theta$, so that for all $\theta \in \Theta$ and for all $Q \in \mathcal{P}^+$ the expectation $\int{\rho_\theta }\mathrm{d}Q$ exists and
		\begin{equation}
			\mathfrak{D} (P_\theta, Q) = \mathfrak{D}^0 (P_\theta) + \mathfrak{D}^1 (Q) + \int \rho_\theta \mathrm{d}Q.
		\end{equation}
\end{definition}

\begin{definition}
	We say that a functional $T_\mathfrak{D}:\mathcal{Q} \rightarrow \Theta$, for $\mathcal{Q}=\mathcal{P}^+ \cup \mathcal{P}_{\text{emp}}$	defines minimum pseudo-distance estimator (min $\mathfrak{D}$-estimator) if $\mathfrak{D}(P_\theta,Q)$ is a decomposable pseudo-distance on $\mathcal{P}\times\mathcal{P}^+$ and parameters $T_\mathfrak{D}(Q) \in \Theta$ minimize $\mathfrak{D}^0 + \int{\rho_\theta}\mathrm{d}Q$, that means
	\begin{equation}
		T_\mathfrak{D}(Q) = \arg\min_{\theta \in \Theta} \left[ \mathfrak{D}^0(P_\theta) + \int{\rho_\theta}\mathrm{d}Q \right] \quad \forall Q \in \mathcal{Q}.
	\end{equation}
\end{definition}
In particular, for $Q = P_n = \frac{1}{n}\sum_{i-1}^n \delta_{X_i} \in \mathcal{P}_{emp}$
\begin{equation}
	\hat{\theta}_{\mathfrak{D},n} =T_\mathfrak{D}(P_n)  = \arg\min_{\theta \in \Theta}\left[ \mathfrak{D}^0(P_\theta) + \dfrac{1}{n} \sum_{i-1}^n \rho_\theta (X_i) \right].
\end{equation}
Every min $\mathfrak{D}$-estimator is Fisher consistent in the sense that
\begin{equation}
	T_\mathfrak{D}(P_{\theta_0}) = \arg\min_{\theta \in \Theta} \mathfrak{D}(P_\theta, P_{\theta_0}) = \theta_0,\quad \forall \theta_0 \in \Theta.
\end{equation}

\begin{theorem}
Let for some $\beta>0$ it holds that
	\begin{equation*}
			p^\beta, q^\beta,\ln{p} \in \mathrm{L}_1(Q), \quad \forall P \in \mathcal{P}, Q \in \mathcal{P^+}.
	\end{equation*}
	Then for all $\alpha$, $0 < \alpha \leq \beta$, and for $P \in \mathcal{P}, \; Q \in \mathcal{P^+} $ the expression
	\begin{equation}
		\mathfrak{R}_\alpha (P,Q) = \dfrac{1}{1+\alpha}\ln{\left( \int{p^\alpha \mathrm{d}P } \right)} +
		\dfrac{1}{\alpha (1+\alpha)}\ln{\left( \int{q^\alpha \mathrm{d}Q } \right)} -
		\dfrac{1}{\alpha} \ln{\left( \int{p^\alpha \mathrm{d}Q } \right)}
	\end{equation}
		represents the family of pseudodistances decomposable in the sense of
	\begin{equation*}
		\mathfrak{R}_\alpha (P,Q) = \mathfrak{R}_\alpha^0 (P) + \mathfrak{R}_\alpha^1 (Q) - \dfrac{1}{\alpha} \ln{\left( \int{p^\alpha \mathrm{d}Q } \right)},
	\end{equation*}	
	where
	\begin{equation*}
		\mathfrak{R}_\alpha^0 (P) = \dfrac{1}{1+\alpha}\ln{\left( \int{p^\alpha \mathrm{d}P } \right)}, \quad \mathfrak{R}_\alpha^1 (Q) = \dfrac{1}{\alpha (1+\alpha)}\ln{\left( \int{q^\alpha \mathrm{d}Q } \right)}.
	\end{equation*}
	Moreover, for $\alpha \searrow 0$ it holds
	\begin{equation*}
		\mathfrak{R}_0 (P,Q) = \lim_{\alpha \searrow 0} \mathfrak{R}_\alpha (P,Q) =  \int{\left( \ln{q} - \ln{p} \right)\mathrm{d}Q}.
	\end{equation*}
\end{theorem}

\noindent The \ren pseudodistance estimator is then determined by

\begin{equation}
	T_{\mathfrak{R}_\alpha}(Q) =
	\begin{cases}
		 \arg \min_{\theta} \left[\frac{1}{1+\alpha} \ln(\int p_\theta^\alpha\mathrm{d}P_\theta) - \frac{1}{\alpha} \ln(\int p_\theta^\alpha\mathrm{d}Q) \right] & \text{for } 0 < \alpha \leq \beta, \\
		 \arg \min_{\theta} \left[- \ln(\int p_\theta\mathrm{d}Q) \right] & \text{for } \alpha = 0.
	\end{cases}	
\end{equation}

We are interested in the estimators, where we replace the hypothetical distribution $P_r$ with the empirical distribution $P_n$. It means that the family of minimum \ren pseudo-distance estimators defined as $\theta_{n,\alpha} = T_{\mathfrak{R}_\alpha}(P_n)$ for $T_{\mathfrak{R}_\alpha}(Q) \in \Theta$ with $Q \in \mathcal{P}^+$ satisfies the condition

\begin{equation}
	\theta_{\alpha,n} =
	\begin{cases}
		\displaystyle{ \arg \max_{\theta \in \Theta} C_\alpha\left( \theta \right)^{-1} \frac{1}{n} \sum_{i=1}^n p_{\theta}^{\alpha}\left( X_i \right) } & \text{for } 0 < \alpha \leq \beta, \\
		\displaystyle{ \arg \max_{\theta \in \Theta}  \frac{1}{n} \sum_{i=1}^n \ln p_{\theta}\left( X_i \right) } & \text{for } \alpha = 0.
	\end{cases}	
	\label{JK-Renyi-estimator_formula}
\end{equation}
		
		\subsection{Application in Normal family}
			
		\subsection{Robustness}
		
		\subsection{Decomposable distances}
	
	\section{Application to specific families}

		\subsection{Laplace}
		
		\subsection{Exponential}
		
		\subsection{Cauchy}
		
		\subsection{Weibull}
	
\chapter{Divergence Trees}

\chapter{Goodness of fit tests}